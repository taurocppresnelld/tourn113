ae = "/app/flux/ae.safetensors"                                                                                                                                      
apply_t5_attn_mask = true                                                                                                                                                      
bucket_no_upscale = true                                                                                                                                                       
bucket_reso_steps = 64                                                                                                                                                         
cache_latents = true
cache_latents_to_disk = true                                                                                                                                                   
caption_extension = ".txt"                                                                                                                                                     
clip_l = "/app/flux/clip_l.safetensors"                                                                                                                              
discrete_flow_shift = 3.1582                                                                                                                                                   
dynamo_backend = "no"                                                                                                                                                          
epoch = 1
full_bf16 = true                                                                                                                                                               
gradient_accumulation_steps = 1                                                                                                                                                
gradient_checkpointing = true                                                                                                                                                  
guidance_scale = 1.0                                                                                                                                                           
highvram = true                                                                                                                                                                
huber_c = 0.1                                                                                                                                                                  
huber_scale = 1                                                                                                                                                                
huber_schedule = "exponential"
huggingface_path_in_repo = "checkpoint"
huggingface_repo_id = ""
huggingface_repo_type = "model"
huggingface_repo_visibility = "public"
huggingface_token = ""                                                                                                                                                         
keep_in_memory = true                # 120GB RAM can handle it
loss_type = "huber"                                                                                                                                                               
lr_scheduler = "cosine_with_restarts"
lr_scheduler_args = []                                                                                                                                                         
lr_scheduler_num_cycles = 2                                                                                                                                                    
lr_scheduler_power = 1                                                                                                                                                         
max_bucket_reso = 2048                                                                                                                                                         
max_data_loader_n_workers = 12       # 12/28 CPUs used
max_timestep = 100                                                                                                                                                            
max_train_steps = 200               
mem_eff_save = true                                                                                                                                                            
min_bucket_reso = 256                                                                                                                                                          
mixed_precision = "bf16"                                                                                                                                                       
model_prediction_type = "raw"                                                                                                                                                  
network_alpha = 128                                                                                                                                                            
network_args = [ "train_double_block_indices=all", "train_single_block_indices=all", "train_t5xxl=True",]                                                                      
network_dim = 128                                                                                                                                                              
network_module = "networks.lora_flux"                                                                                                                                          
noise_offset_type = "Original"                                                                                                                                                 
optimizer_args = ["weight_decay=0.01"]
optimizer_type = "Lion8bit"          # Faster than Adafactor
output_dir = "/app/outputs"                                                                                                                                          
output_name = "last"                                                                                                                                                 
persistent_data_loader_workers = true
pretrained_model_name_or_path = "/app/flux/unet.safetensors"                                                                                                                  
prior_loss_weight = 1                                                                                                                                                          
resolution = "768,768"                                                                                                                                                       
sample_prompts = ""                                                                                                                    
sample_sampler = "euler_a"                                                                                                                                                     
#save_every_n_epochs = 25                                                                                                                                                       
save_every_n_steps = 50
save_model_as = "safetensors"                                                                                                                                                  
save_precision = "float"                                                                                                                                                       
seed = 1                                                                                                                                                                       
t5xxl = "/app/flux/t5xxl_fp16.safetensors"                                                                                                                           
t5xxl_max_token_length = 512                                                                                                                                                   
text_encoder_lr = [1e-5, 1e-5]       # Slightly unfrozen
timestep_sampling = "sigmoid"                                                                                                                                                  
train_batch_size = 2                 # Max for A100 80GB
train_data_dir = ""                                                                                                                               
unet_lr = 1e-4                       # Higher for speed
vae_batch_size = 32                  # Faster caching
wandb_run_name = "last"                                                                                                                                              
xformers = true
validate_dataset = true
